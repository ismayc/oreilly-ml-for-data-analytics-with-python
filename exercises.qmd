---
title: 'Walkthroughs and Exercises for *Machine Learning for Data Analytics with Python*'  
author: "Dr. Chester Ismay"  
format:
  html:
    theme: flatly
    toc: true
    toc-floating: true 
engine: knitr  
---

# Intro: Getting Started with Machine Learning for Data-Driven Decisions

## Walkthrough #1: Setting Up the Python Environment for ML

If you haven't already installed Python, Jupyter, and the necessary packages, there are instructions on the course repo in the README to do so [here](https://github.com/ismayc/oreilly-data-analysis-with-python/blob/main/README.md). 

You can also install the packages directly in a Jupyter notebook with

```{python}
!pip install pandas seaborn matplotlib scikit-learn mlxtend
```

If you aren't able to do this on your machine, you may want to check out [Google Colab](https://colab.research.google.com/). It's a free service that allows you to run Jupyter notebooks in the cloud. Alternatively, I've set up some temporary notebooks on Binder [here](https://mybinder.org/v2/gh/ismayc/oreilly-ml-for-data-analytics-with-python/HEAD?urlpath=%2Fdoc%2Ftree%2Fexercises.ipynb) that you can work with online as well.

Run the following code to check that each of the needed packages are installed. If you get an error, you may need to install the package(s) again.


```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from mlxtend.frequent_patterns import apriori, association_rules
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, cross_validate
from sklearn.model_selection import GridSearchCV
```

```{python}
# Load dataset
telco_churn_raw = pd.read_csv('telco-customer-churn.csv')
```


## Exercise #1: Setting Up the Python Environment

By completing this exercise, you will be able to  

- Import necessary Python packages  
- Check for successful package loading  
- Load datasets into Python

Follow the instructions above in Walkthrough #1 to check for correct installation 
of necessary packages.

```{python}
# Load dataset
marketing_campaign_raw = pd.read_csv('marketing_campaign.csv')
```

---

# Module 1: Data Understanding and Preprocessing for Machine Learning


## Walkthrough #2: Exploring and Preprocessing Data with Pandas & Seaborn

### Inspect a dataset using Pandas


```{python}
# Inspect data structure

```

```{python}
# Check for missing values

```

```{python}
# Check for duplicate rows

```

### Handle missing values and clean data

```{python}
# Make a copy of the data to fix and clean

```

```{python}
# Handle missing values

```

```{python}
# Standardize column formats (e.g., convert Yes/No to binary for a few columns)

```

```{python}
# Summarize statistics

```

### Create visualizations to identify key business trends

```{python}
# Visualize Churn by tenure in a violin plot

```

```{python}
# Visualize the distribution of AvgServiceUsageScore

```

```{python}
# Visualize the distribution of MonthlyCharges

```

```{python}
# Look at the relationship between AvgServiceUsageScore and MonthlyCharges

```

```{python}
# Visualize the relationship between MonthlyCharges and Churn

```


---



## Exercise #2: Exploring and Preprocessing Data with Pandas & Seaborn

### Inspect a dataset using Pandas

```{python}
# Inspect data structure

```

```{python}
# Check for missing values

```

```{python}
# Check for duplicate rows

```

### Handle missing values and clean data

```{python}
# Create a clean working copy

```

```{python}
# Handle missing values of Income

```

```{python}
# Add new features for TotalChildren and 
# TotalSpent = sum of all spending (Mnt) categories

```

```{python}
# Convert Dt_Customer to datetime

```

```{python}
# Summarize structure

```


### Create visualizations to identify key business trends

```{python}
# Violin plot of Income by Campaign Response

```

```{python}
# Histogram of Total Spent

```

```{python}
# Relationship between Income and Total Spent colored by Campaign Response

```

```{python}
# Bar plot of Campaign Response by Education Level

```

---


# Module 2: Supervised Learning for Business Decisions

## Walkthrough #3: Build a Regression Model for Pricing Optimization

### Split the data into training and validation sets

```{python}
# Choose AvgServiceUsageScore as the predictor and MonthlyCharges as the target

```

```{python}
# Scale the predictor

```

```{python}
# Make an 80/20 split

```

### Train a linear regression model

```{python}
# Instantiate and fit the model

```

```{python}
# Print the intercept and coefficient

```

### Evaluate model performance on the validation set

```{python}
# Predict on the validation set

```

```{python}
# Evaluate model performance with R^2 and MAE

```

```{python}
# Print these evaluation metrics

```

---

## Exercise #3: Build a Regression Model for Pricing Optimization

### Split the data into training and validation sets

```{python}
# Predictor (Income) and target (TotalSpent)

```

```{python}
# Standardize the predictor

```

```{python}
# Train-test split

```


### Train a linear regression model

```{python}
# Instantiate and fit the model

```

```{python}
# Print the intercept and coefficient

```


### Evaluate model performance on the validation set

```{python}
# Predict on the validation set

```

```{python}
# Get R^2 and MAE

```

```{python}
# Print these evaluation metrics


```


---

## Walkthrough #4: Implement a Classification Model for Customer Churn

### Split the data into training and validation sets

```{python}
# Select relevant features and set Churn to target

```

Scaling is not as important for tree-based models.

```{python}

```

### Train a Random Forest classification model

```{python}
# Train Random Forest classifier

```

### Evaluate model performance on the validation set

```{python}
# Predict on the validation set

```

```{python}
# Print evaluation metrics (Accuracy, Precision, Recall)

```

```{python}
# Find confusion matrix

```

```{python}
# Set labels for confusion matrix and turn into DataFrame

```

```{python}
# Print formatted confusion matrix

```

---

## Exercise #4: Implement a Classification Model for Customer Churn

### Split the data into training and validation sets

```{python}
# Select relevant features (Income, TotalSpent, TotalChildren) and 
# set Response to target

```

```{python}
# Split into training and validation sets

```


### Train a Random Forest classification model

```{python}
# Train Random Forest classifier

```


### Evaluate model performance on the validation set

```{python}
# Predict on the validation set

```

```{python}
# Get accuracy, precision, and recall

```

```{python}
# Set labels and turn confusion matrix into DataFrame

# Print formatted confusion matrix

```


---

# Module 3: Unsupervised Learning and Pattern Discovery in Business

## Walkthrough #5: Exploring K-Means Clustering for Customer Segmentation

### Apply K-Means clustering to segment customers

```{python}
# Set ContractType as a numerical variable


# Select relevant features


# Standardize features

```

### Determine the optimal number of clusters using the Elbow Method

```{python}
# Set inertia as an empty list


# Loop through k=1 to k=10


# Fit KMeans for each k and append inertia to the list


# Plot the inertia values against the number of clusters

```

### Verify using the silhouette score (optional but recommended)

```{python, warnings=FALSE}
# Evaluate silhouette scores for k=3 to k=6



    # Fit KMeans for each k and calculate silhouette score
    
    
    # Fit KMeans and get labels
    
    
    # Calculate silhouette score
    
    
    # Store silhouette score
    

# Print silhouette scores

```

### Fit K-means and assign cluster labels to each customer

```{python}
# Let's assume the elbow suggested k=3


# Fit KMeans with optimal k

# Create a new column in the DataFrame for cluster labels

```

### Visualize customer segments using a 2D plot

```{python}

# Visualize clusters in 2D space (using tenure and AvgServiceUsageScore)

```

---

## Exercise #5: Exploring K-Means Clustering for Customer Segmentation

### Apply K-Means clustering to segment customers

```{python}
# Select relevant features (Income, TotalSpent, TotalChildren)


# Standardize features

```

### Determine the optimal number of clusters using the Elbow Method

```{python}
# Append inertia values for k=1 to k=10 after fitting KMeans

# Plot Elbow Method for Optimal K using inertia values

```

### Verify using the silhouette score (optional but recommended)

```{python}
# Calculate silhouette scores for k=3 to k=6

```

### Fit K-means and assign cluster labels to each customer

```{python}
# Determine optimal k from the elbow method + silhouette score

# Run KMeans with optimal k

# Create a new column in the DataFrame for cluster labels

```

### Visualize customer segments using a 2D plot

```{python}
# Look at clusters using TotalChildren and TotalSpent

# Uncomment to set x-axis to integer ticks only
# plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(1))
```

---

## Walkthrough #6: Market Basket Analysis with Apriori Algorithm

### Prepare transactional data (services as items)

```{python}
# Select binary service columns to act like "products"


# Convert service columns to boolean (preference of apriori() function)

```

### Apply the Apriori algorithm to identify frequent itemsets

```{python}
# Use apriori to find frequent itemsets with min_support=0.2

```

### Generate association rules from frequent itemsets

```{python}
# Generate association rules with min_threshold=0.6 using confidence metric


# Return columns to extract insights

```

### Interpret insights

Key Metrics:



Interpretation of Key Rules:


Business insights:  


General pattern:


---

## Exercise #6: Market Basket Analysis with Apriori Algorithm

### Prepare transactional data (product categories as items)

```{python}
# Select binary columns representing product purchases

# Convert product columns to binary: 1 if any amount was spent, else 0

# Rename columns for cleaner output

# Set columns as booleans

```


### Apply the Apriori algorithm to identify frequent itemsets

```{python}
# Run apriori algorithm to find frequent itemsets with min_support=0.2


# Sort itemset values by support

```


### Generate association rules from frequent itemsets

```{python}

# Generate association rules with min_threshold=0.6 using confidence metric

# Sort rules by lift

```


### Interpret insights


---

# Module 4: Implementing and Evaluating ML Models

## Walkthrough #7: Exploring Cross-Validation for Model Evaluation

### Split data into training and validation sets

```{python}
# Features (AvgServiceUsageScore, tenure, MonthlyCharges) and target (Churn)


# Scale the features since logistic regression is a linear model


# Split into train and test sets (80% train, 20% test)

```

### Train a classification model using logistic regression

```{python}
# Initialize logistic regression model

```

### Apply k-fold cross-validation to evaluate model performance

```{python}
# 5-fold cross-validate with multiple metrics (accuracy, precision, recall, f1)

```

### Compare metrics across folds

```{python}
# Average performance across folds

```

---

## Exercise #7: Exploring Cross-Validation for Model Evaluation

### Split data into training and validation sets

```{python}
# Features (Income, TotalSpent, TotalChildren) and target Response


# Scale features


# Train-test split

```

### Train a classification model using logistic regression

```{python}
# Initialize logistic regression model

```

### Apply k-fold cross-validation to evaluate model performance

```{python}
# 5-fold cross-validation with multiple metrics

```


### Compare metrics across folds

```{python}
# Average performance across folds

```

---

## Walkthrough #8: Hyperparameter Tuning with Grid Search

### Train a Random Forest classifier

```{python}
# Features (AvgServiceUsageScore, tenure, MonthlyCharges) and target Churn


# Initialize Random Forest

```

### Apply grid search to find optimal hyperparameters

```{python gridsearch}
# Define the hyperparameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [4, 8],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

# GridSearchCV setup


# Run grid search

```

### Evaluate model improvement using accuracy and recall

```{python}
# Predict using the best model

# Show accuracy and recall for best model

```

### Interpret the best hyperparameter combination

```{python}
# Print the best hyperparameters found

```

---

## Exercise #8: Hyperparameter Tuning with Grid Search

### Train a Random Forest classifier

```{python}
# Features (Income, TotalSpent, TotalChildren) and target Response


# Initialize Random Forest

```


### Apply grid search to find optimal hyperparameters

```{python}
# Define the hyperparameter grid
param_grid_m = {
    'n_estimators': [100, 200],
    'max_depth': [4, 8],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

# GridSearchCV setup with 5-fold cross-validation


# Run grid search

```


### Evaluate model improvement using accuracy and recall

```{python}
# Predict using the best model


# Show accuracy and recall for best model

```

### Interpret the best hyperparameter combination

```{python}
# Print the best hyperparameters found

```

